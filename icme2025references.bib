@misc{wangDisentangledRepresentationLearning2024,
  title        = {Disentangled {{Representation Learning}}},
  author       = {Wang, Xin and Chen, Hong and Tang, Si'ao and Wu, Zihao and Zhu, Wenwu},
  year         = {2024},
  eprint       = {2211.11695},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2211.11695}
}

@inproceedings{liuMultitaskAdversarialNetwork2018,
  title     = {Multi-Task {{Adversarial Network}} for {{Disentangled Feature Learning}}},
  booktitle = {2018 {{IEEE}}/{{CVF Conference}} on {{Computer Vision}} and {{Pattern Recognition}}},
  author    = {Liu, Yang and Wang, Zhaowen and Jin, Hailin and Wassell, Ian},
  year      = {2018},
  pages     = {3743--3751},
  address   = {Salt Lake City, UT, USA},
  doi       = {10.1109/CVPR.2018.00394},
  isbn      = {978-1-5386-6420-9}
}

@misc{materzynskaDisentanglingVisualWritten2022,
  title        = {Disentangling Visual and Written Concepts in {{CLIP}}},
  author       = {Materzynska, Joanna and Torralba, Antonio and Bau, David},
  year         = {2022},
  eprint       = {2206.07835},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2206.07835}
}

@misc{chengDisentangledFeatureRepresentation2021,
  title        = {Disentangled {{Feature Representation}} for {{Few-shot Image Classification}}},
  author       = {Cheng, Hao and Wang, Yufei and Li, Haoliang and Kot, Alex C. and Wen, Bihan},
  year         = {2021},
  eprint       = {2109.12548},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2109.12548}
}

@inproceedings{liDisentanglingIdentityFeatures2024,
  title     = {Disentangling {{Identity Features}} from {{Interference Factors}} for {{Cloth-Changing Person Re-identification}}},
  booktitle = {Proceedings of the 32nd {{ACM International Conference}} on {{Multimedia}}},
  author    = {Li, Yubo and others},
  year      = {2024},
  pages     = {2252--2261},
  address   = {Melbourne VIC Australia},
  doi       = {10.1145/3664647.3680823},
  isbn      = {979-8-4007-0686-8}
}

@misc{azadActivityBiometricsPersonIdentification2024,
  title        = {Activity-{{Biometrics}}: {{Person Identification}} from {{Daily Activities}}},
  author       = {Azad, Shehreen and Rawat, Yogesh Singh},
  year         = {2024},
  eprint       = {2403.17360},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2403.17360}
}

@misc{cuiProFDPromptGuidedFeature2024,
  title        = {{{ProFD}}: {{Prompt-Guided Feature Disentangling}} for {{Occluded Person Re-Identification}}},
  author       = {Cui, Can and others},
  year         = {2024},
  eprint       = {2409.20081},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2409.20081},
  annotation   = {TLDR: A Prompt-guided Feature Disentangling method (ProFD), which leverages the rich pre-trained knowledge in the textual modality facilitate model to generate well-aligned part features and adopts a hybrid-attention decoder, ensuring spatial and semantic consistency during the decoding process to minimize noise impact.}
}

@misc{yinGraFTGradualFusion2023,
  title        = {{{GraFT}}: {{Gradual Fusion Transformer}} for {{Multimodal Re-Identification}}},
  author       = {Yin, Haoli and Li, Jiayao and Schiller, Eva and McDermott, Luke and Cummings, Daniel},
  year         = {2023},
  eprint       = {2310.16856},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2310.16856},
  annotation   = {TLDR: This work introduces the GraFT, a novel training paradigm combined with an augmented triplet loss, optimizing the ReID feature embedding space and demonstrating that GraFT consistently surpasses established multimodal ReID benchmarks.}
}

@misc{kimExtendingCLIPImageText2024,
  title        = {Extending {{CLIP}}'s {{Image-Text Alignment}} to {{Referring Image Segmentation}}},
  author       = {Kim, Seoyeon and Kang, Minguk and Kim, Dongwon and Park, Jaesik and Kwak, Suha},
  year         = {2024},
  eprint       = {2306.08498},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2306.08498}
}

@misc{li2023learninggraphneuralnetwork,
  title        = {Learning a {{Graph Neural Network}} with {{Cross Modality Interaction}} for {{Image Fusion}}},
  author       = {Li, Jiawei and Chen, Jiansheng and Liu, Jinyuan and Ma, Huimin},
  year         = {2023},
  eprint       = {2308.03256},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2308.03256}
}

@article{bukhariLanguageVisionBased2023,
  title   = {Language and Vision Based {Person Re-Identification} for Surveillance Systems Using Deep Learning with {{LIP}} Layers},
  author  = {Bukhari, Maryam and others},
  year    = {2023},
  journal = {Image and Vision Computing},
  volume  = {132},
  pages   = {104658},
  doi     = {10.1016/j.imavis.2023.104658}
}

@misc{dingSemanticallySelfAlignedNetwork2021,
  title        = {Semantically {{Self-Aligned Network}} for {{Text-to-Image Part-aware Person Re-identification}}},
  author       = {Ding, Zefeng and Ding, Changxing and Shao, Zhiyin and Tao, Dacheng},
  year         = {2021},
  eprint       = {2107.12666},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2107.12666}
}

@article{dosovitskiy2020image,
  title   = {An image is worth 16x16 words: Transformers for image recognition at scale},
  author  = {Dosovitskiy, Alexey},
  journal = {arXiv preprint arXiv:2010.11929},
  year    = {2020}
}

@misc{gaoContextualNonLocalAlignment2021,
  title        = {Contextual {{Non-Local Alignment}} over {{Full-Scale Representation}} for {{Text-Based Person Search}}},
  author       = {Gao, Chenyang and others},
  year         = {2021},
  eprint       = {2101.03036},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2101.03036}
}

@article{galiyawalaPersonRetrievalSurveillance2021,
  title        = {Person {{Retrieval}} in {{Surveillance Using Textual Query}}: {{A Review}}},
  author       = {Galiyawala, Hiren and Raval, Mehul S.},
  year         = {2021},
  journal      = {arXiv},
  eprint       = {2105.02414},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2105.02414}
}

@misc{wangLearningDeepStructurePreserving2016,
  title        = {Learning {{Deep Structure-Preserving Image-Text Embeddings}}},
  author       = {Wang, Liwei and Li, Yin and Lazebnik, Svetlana},
  year         = {2016},
  eprint       = {1511.06078},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.1511.06078}
}

@misc{wangViTAAVisualTextualAttributes2020,
  title        = {{{ViTAA}}: {{Visual-Textual Attributes Alignment}} in {{Person Search}} by {{Natural Language}}},
  author       = {Wang, Zhe and Fang, Zhiyuan and Wang, Jun and Yang, Yezhou},
  year         = {2020},
  eprint       = {2005.07327},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2005.07327}
}

@misc{yaoFILIPFinegrainedInteractive2021,
  title        = {{{FILIP}}: {{Fine-grained Interactive Language-Image Pre-Training}}},
  author       = {Yao, Lewei and others},
  year         = {2021},
  eprint       = {2111.07783},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2111.07783}
}

@misc{jiangCrossModalImplicitRelation2023,
  title        = {Cross-{{Modal Implicit Relation Reasoning}} and {{Aligning}} for {{Text-to-Image Person Retrieval}}},
  author       = {Jiang, Ding and Ye, Mang},
  year         = {2023},
  eprint       = {2303.12501},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2303.12501}
}

@inproceedings{li2017person,
  title     = {Person search with natural language description},
  author    = {Li, Shuang and Xiao, Tong and Li, Hongsheng and Zhou, Bolei and Yue, Dayu and Wang, Xiaogang},
  booktitle = {Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages     = {1970--1979},
  year      = {2017}
}

@misc{liPromptDecouplingTexttoImage2024,
  title        = {Prompt {{Decoupling}} for {{Text-to-Image Person Re-identification}}},
  author       = {Li, Weihao and Tan, Lei and Dai, Pingyang and Zhang, Yan},
  year         = {2024},
  eprint       = {2401.02173},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2401.02173},
  annotation   = {TLDR: This work introduces the prompt tuning strategy to enable domain adaptation and proposes a two-stage training approach to disentangle domain adaptation from task adaptation for text-to-image person re-identification.}
}

@article{liuCausalityInspiredInvariantRepresentation2024,
  title      = {Causality-{{Inspired Invariant Representation Learning}} for {{Text-Based Person Retrieval}}},
  author     = {Liu, Yu and Qin, Guihe and Chen, Haipeng and Cheng, Zhiyong and Yang, Xun},
  year       = {2024},
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume     = {38},
  pages      = {14052--14060},
  doi        = {10.1609/aaai.v38i12.29314},
  annotation = {TLDR: This work pioneer the observation of TPR from a causal view, and proposes an Invariant Representation Learning method for TPR (IRLT), that enforces the visual representations to satisfy the two critical properties of causal/non-causal factors.}
}

@misc{yanPrototypicalPromptingTexttoimage2024,
  title        = {Prototypical {{Prompting}} for {{Text-to-image Person Re-identification}}},
  author       = {Yan, Shuanglin and Liu, Jun and Dong, Neng and Zhang, Liyan and Tang, Jinhui},
  year         = {2024},
  eprint       = {2409.09427},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2409.09427}
}

@misc{qinNoisyCorrespondenceLearningTexttoImage2024,
  title        = {Noisy-{{Correspondence Learning}} for {{Text-to-Image Person Re-identification}}},
  author       = {Qin, Yang and others},
  year         = {2024},
  eprint       = {2308.09911},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2308.09911}
}

@misc{zhengCPCLCrossModalPrototypical2024,
  title        = {{{CPCL}}: {{Cross-Modal Prototypical Contrastive Learning}} for {{Weakly Supervised Text-based Person Re-Identification}}},
  author       = {Zheng, Yanwei and others},
  year         = {2024},
  eprint       = {2401.10011},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2401.10011},
  annotation   = {TLDR: The proposed Cross-Modal Prototypical Contrastive Learning (CPCL) method introduces the CLIP model to weakly supervised TPRe-ID for the first time, mapping visual and textual instances into a shared latent space and captures associations between heterogeneous modalities of image-text pairs belonging to the same person.}
}

@misc{fengKnowledgeGuidedDynamicModality2024,
  title        = {Knowledge-{{Guided Dynamic Modality Attention Fusion Framework}} for {{Multimodal Sentiment Analysis}}},
  author       = {Feng, Xinyu and others},
  year         = {2024},
  eprint       = {2410.04491},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2410.04491},
  annotation   = {TLDR: A Knowledge-Guided Dynamic Modality Attention Fusion Framework (KuDA) for multimodal sentiment analysis that achieves state-of-the-art performance and is able to adapt to different scenarios of dominant modality.}
}

@misc{yanCLIPDrivenFinegrainedTextImage2022,
  title        = {{{CLIP-Driven Fine-grained Text-Image Person Re-identification}}},
  author       = {Yan, Shuanglin and Dong, Neng and Zhang, Liyan and Tang, Jinhui},
  year         = {2022},
  eprint       = {2210.10276},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2210.10276}
}

@misc{shuSeeFinerSeeMore2022,
  title        = {See {{Finer}}, {{See More}}: {{Implicit Modality Alignment}} for {{Text-based Person Retrieval}}},
  author       = {Shu, Xiujun and others},
  year         = {2022},
  eprint       = {2208.08608},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2208.08608},
  annotation   = {TLDR: An Implicit Visual-Textual (IVT) framework for text-based person retrieval is introduced and two implicit semantic alignment paradigms are proposed: multi-level alignment (MLA) and bidirectional mask modeling (BMM).}
}

@misc{zuoUFineBenchTowardsTextbased2024,
  title        = {{{UFineBench}}: {{Towards Text-based Person Retrieval}} with {{Ultra-fine Granularity}}},
  author       = {Zuo, Jialong and others},
  year         = {2024},
  eprint       = {2312.03441},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2312.03441}
}

@misc{zhuDSSLDeepSurroundingsperson2021,
  title        = {{{DSSL}}: {{Deep Surroundings-person Separation Learning}} for {{Text-based Person Retrieval}}},
  author       = {Zhu, Aichun and others},
  year         = {2021},
  eprint       = {2109.05534},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2109.05534}
}

@misc{jiangModelingThousandsHuman2025,
  title        = {Modeling {{Thousands}} of {{Human Annotators}} for {{Generalizable Text-to-Image Person Re-identification}}},
  author       = {Jiang, Jiayu and others},
  year         = {2025},
  eprint       = {2503.09962},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2503.09962}
}

@misc{schmidtRobustCanonicalizationBootstrapped2025,
  title        = {Robust {{Canonicalization}} through {{Bootstrapped Data Re-Alignment}}},
  author       = {Schmidt, Johann and Stober, Sebastian},
  year         = {2025},
  eprint       = {2510.08178},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2510.08178}
}

@misc{cretuEvaluatingConceptFiltering2025,
  title        = {Evaluating {{Concept Filtering Defenses}} against {{Child Sexual Abuse Material Generation}} by {{Text-to-Image Models}}},
  author       = {Cretu, Ana-Maria and Kireev, Klim and Abdalla, Amro and Obinna, Wisdom and Meier, Raphael and Bargal, Sarah Adel and Redmiles, Elissa M. and Troncoso, Carmela},
  year         = 2025,
  eprint       = {2512.05707},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2512.05707}
}

@misc{fedeleSpaceControlIntroducingTestTime2025,
  title        = {{{SpaceControl}}: {{Introducing Test-Time Spatial Control}} to {{3D Generative Modeling}}},
  author       = {Fedele, Elisabetta and Engelmann, Francis and Huang, Ian and Litany, Or and Pollefeys, Marc and Guibas, Leonidas},
  year         = 2025,
  eprint       = {2512.05343},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2512.05343}
}

@misc{gongBaseDetailFeatureLearning2025,
  title        = {Base-{{Detail Feature Learning Framework}} for {{Visible-Infrared Person Re-Identification}}},
  author       = {Gong, Zhihao and Wu, Lian and Xu, Yong},
  year         = 2025,
  eprint       = {2505.03286},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2505.03286},
  annotation   = {TLDR: A Base-Detail Feature Learning Framework (BDLF) is proposed that enhances the learning of both base and detail knowledge, thereby capitalizing on both modality-shared and modality-specific information.}
}

@misc{jiaAdaptiveIlluminationInvariantSynergistic2025,
  title        = {Adaptive {{Illumination-Invariant Synergistic Feature Integration}} in a {{Stratified Granular Framework}} for {{Visible-Infrared Re-Identification}}},
  author       = {Jia, Yuheng and Armour, Wesley},
  year         = 2025,
  eprint       = {2502.21163},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2502.21163},
  annotation   = {TLDR: AMINet employs multi-granularity feature extraction to capture comprehensive identity attributes from both full-body and upper-body images, improving robustness against occlusions and background clutter and effectively bridging the RGB-IR modality gap.}
}

@misc{luARGUSDefendingMultimodal2025,
  title        = {{{ARGUS}}: {{Defending Against Multimodal Indirect Prompt Injection}} via {{Steering Instruction-Following Behavior}}},
  author       = {Lu, Weikai and Zeng, Ziqian and Zhang, Kehua and Li, Haoran and Zhuang, Huiping and Wang, Ruidong and Chen, Cen and Peng, Hao},
  year         = 2025,
  eprint       = {2512.05745},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2512.05745}
}

@article{zhangMultiStageAuxiliaryLearning2024,
  title      = {Multi-{{Stage Auxiliary Learning}} for {{Visible-Infrared Person Re-Identification}}},
  author     = {Zhang, Huadong and Cheng, Shuli and Du, Anyu},
  year       = 2024,
  journal    = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume     = {34},
  pages      = {12032--12047},
  doi        = {10.1109/TCSVT.2024.3425536},
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: A novel Multi-Stage Auxiliary Learning strategy called MSALNet is proposed, which demonstrates MSALNet's superior performance over most existing methods on two mainstream VI-ReID datasets and effectively saves computational cost.}
}

@article{zhangVisibleInfraredPersonReIdentification2025,
  title      = {Visible-{{Infrared Person Re-Identification With Real-World Label Noise}}},
  author     = {Zhang, Ruiheng and Cao, Zhe and Huang, Yan and Yang, Shuo and Xu, Lixin and Xu, Min},
  year       = 2025,
  journal    = {IEEE Transactions on Circuits and Systems for Video Technology},
  volume     = {35},
  pages      = {4857--4869},
  doi        = {10.1109/TCSVT.2025.3526449},
  copyright  = {https://ieeexplore.ieee.org/Xplorehelp/downloads/license-information/IEEE.html},
  annotation = {TLDR: A Robust Hybrid Loss function (RHL) is developed that enables targeted recognition and retrieval optimization through a more fine-grained division of the noisy dataset, and re-annotated a real-world dataset, SYSU-Real.}
}

@misc{zhangWeaklySupervisedVisibleInfrared2025,
  title        = {Weakly {{Supervised Visible-Infrared Person Re-Identification}} via {{Heterogeneous Expert Collaborative Consistency Learning}}},
  author       = {Zhang, Yafei and Kong, Lingqi and Li, Huafeng and Wen, Jie},
  year         = 2025,
  eprint       = {2507.12942},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2507.12942},
  annotation   = {TLDR: A weakly supervised cross-modal person ReID method that uses only single-modal sample identity labels, addressing scenarios where cross-modal identity labels are unavailable is explored, and a cross-modal relationship fusion mechanism that effectively integrates predictions from different experts is designed.}
}

@misc{zhaoVRSAJailbreakingMultimodal2025a,
  title        = {{{VRSA}}: {{Jailbreaking Multimodal Large Language Models}} through {{Visual Reasoning Sequential Attack}}},
  author       = {Zhao, Shiji and Xiong, Shukun and Huang, Yao and Jin, Yan and Wu, Zhenyu and Guan, Jiyang and Duan, Ranjie and Tao, Jialing and Xue, Hui and Wei, Xingxing},
  year         = 2025,
  eprint       = {2512.05853},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2512.05853}
}

@misc{daiDiffusionbasedSyntheticData2025,
  title        = {Diffusion-Based {{Synthetic Data Generation}} for {{Visible-Infrared Person Re-Identification}}},
  author       = {Dai, Wenbo and Lu, Lijing and Li, Zhihang},
  year         = 2025,
  eprint       = {2503.12472},
  primaryclass = {cs},
  doi          = {10.48550/arXiv.2503.12472},
  annotation   = {TLDR: A novel data generation framework, dubbed Diffusion-based VI-ReID data Expansion (DiVE), that automatically obtain massive RGB-IR paired images with identity preserving by decoupling identity and modality to improve the performance of VI-ReID models.}
}

@article{liInfraredVisibleCrossModalPerson2020,
  title      = {Infrared-{{Visible Cross-Modal Person Re-Identification}} with an {{X Modality}}},
  author     = {Li, Diangang and Wei, Xing and Hong, Xiaopeng and Gong, Yihong},
  year       = 2020,
  journal    = {Proceedings of the AAAI Conference on Artificial Intelligence},
  volume     = {34},
  pages      = {4610--4617},
  doi        = {10.1609/aaai.v34i04.5891},
  copyright  = {https://www.aaai.org},
  annotation = {TLDR: An X-Infrared-Visible (XIV) ReID cross-modal learning framework is proposed, which achieves an absolute gain of over 7\% in terms of rank 1 and mAP even compared with the latest state-of-the-art methods.}
}

@inproceedings{radford2021learning,
  title        = {Learning transferable visual models from natural language supervision},
  author       = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and others},
  booktitle    = {International conference on machine learning},
  pages        = {8748--8763},
  year         = {2021},
  organization = {PmLR}
}

@inproceedings{gretton2005measuring,
  title        = {Measuring statistical dependence with Hilbert-Schmidt norms},
  author       = {Gretton, Arthur and Bousquet, Olivier and Smola, Alex and Sch{\"o}lkopf, Bernhard},
  booktitle    = {International conference on algorithmic learning theory},
  pages        = {63--77},
  year         = {2005},
  organization = {Springer}
}

@inproceedings{gu2024mamba,
  title     = {Mamba: Linear-time sequence modeling with selective state spaces},
  author    = {Gu, Albert and Dao, Tri},
  booktitle = {First conference on language modeling},
  year      = {2024}
}