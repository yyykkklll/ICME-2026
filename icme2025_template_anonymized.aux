\relax 
\citation{shaoLearningGranularityUnifiedRepresentations2022,dingSemanticallySelfAlignedNetwork2021}
\citation{bukhariLanguageVisionBased2023}
\citation{galiyawalaPersonRetrievalSurveillance2021}
\citation{wangLearningDeepStructurePreserving2016,wangLanguagePersonSearch2019}
\citation{gaoContextualNonLocalAlignment2021,wangCAIBCCapturingAllround2022}
\citation{wangViTAAVisualTextualAttributes2020,dingSemanticallySelfAlignedNetwork2021}
\citation{jiangCrossModalImplicitRelation2023,shaoLearningGranularityUnifiedRepresentations2022}
\citation{dosovitskiyImageWorth16x162021,tanHarnessingPowerMLLMs2024,yangUnifiedTextbasedPerson2023}
\citation{radfordLearningTransferableVisual2021,yaoFILIPFinegrainedInteractive2021,niuLLMLocBootstrapSingleimage2025}
\citation{jiangModelingThousandsHuman2025}
\@writefile{toc}{\contentsline {section}{\numberline {I}Introduction}{1}{}\protected@file@percent }
\newlabel{sec:intro}{{I}{1}{}{section.1}{}}
\newlabel{sec:intro@cref}{{[section][1][]I}{[1][1][]1}{}{}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Comparison of ReID methods. (a) Traditional: Direct fusion of image and text features without distinguishing identity from non-identity information limits alignment. (b) Proposed (BDAM): Introduces a decoupling module that aligns separated identity/clothing features with MLLM-generated descriptions for fine-grained matching.}}{1}{}\protected@file@percent }
\newlabel{fig:figure1}{{1}{1}{}{figure.1}{}}
\newlabel{fig:figure1@cref}{{[figure][1][]1}{[1][1][]1}{}{}{}}
\citation{wangDisentangledRepresentationLearning2024}
\citation{liuMultitaskAdversarialNetwork2018}
\citation{chengDisentangledFeatureRepresentation2021}
\citation{materzynskaDisentanglingVisualWritten2022}
\citation{liDisentanglingIdentityFeatures2024,azadActivityBiometricsPersonIdentification2024}
\citation{liDisentanglingIdentityFeatures2024}
\citation{schmidtRobustCanonicalizationBootstrapped2025}
\citation{yinGraFTGradualFusion2023}
\citation{kimExtendingCLIPImageText2024}
\citation{fengKnowledgeGuidedDynamicModality2024}
\citation{li2019graph}
\citation{radfordLearningTransferableVisual2021}
\citation{dosovitskiyImageWorth16x162021}
\@writefile{toc}{\contentsline {section}{\numberline {II}Related Work}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-A}}Feature Disentanglement}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {II-B}}Feature Fusion}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {section}{\numberline {III}Method}{2}{}\protected@file@percent }
\newlabel{sec:method}{{III}{2}{}{section.3}{}}
\newlabel{sec:method@cref}{{[section][3][]III}{[1][2][]2}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-A}}Overview}{2}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-B}}Bidirectional Decoupled Alignment Module}{2}{}\protected@file@percent }
\citation{jiangModelingThousandsHuman2025}
\citation{hanafiFastDBSCANAlgorithm2022}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces First, we use an MLLM with predefined prompts to generate identity-related and clothing-related descriptions from the pedestrian image, which are encoded as $f^t_{\mathrm  {clo}}$ and $f^t_{\mathrm  {id}}$, respectively. Then, the input pedestrian image is encoded into a visual feature $f_i$, which still resides in an entangled feature space. Subsequently, the BDAM module, composed of a dual-branch attention mechanism, decouples $f_i$ into identity feature $f_{\mathrm  {id}}$ and non-identity feature $f_{\mathrm  {clo}}$. The decoupling process is supervised and optimized through disentanglement loss and corresponding textual descriptions via contrastive learning. Finally, the fusion module integrates $f_{\mathrm  {id}}$ and $f_{\mathrm  {id}}^t$ to generate the final fused feature representation.}}{3}{}\protected@file@percent }
\newlabel{fig:figure2}{{2}{3}{}{figure.2}{}}
\newlabel{fig:figure2@cref}{{[figure][2][]2}{[1][2][]3}{}{}{}}
\newlabel{eq:aln}{{1}{3}{}{equation.1}{}}
\newlabel{eq:aln@cref}{{[equation][1][]1}{[1][3][]3}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-C}}Semantic enhancement}{3}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-D}}Feature Fusion}{4}{}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {III-E}}Loss Function}{4}{}\protected@file@percent }
\newlabel{eq:info}{{3}{4}{}{equation.3}{}}
\newlabel{eq:info@cref}{{[equation][3][]3}{[1][4][]4}{}{}{}}
\newlabel{eq:triplet}{{4}{4}{}{equation.4}{}}
\newlabel{eq:triplet@cref}{{[equation][4][]4}{[1][4][]4}{}{}{}}
\newlabel{eq:gradnorm}{{5}{4}{}{equation.5}{}}
\newlabel{eq:gradnorm@cref}{{[equation][5][]5}{[1][4][]4}{}{}{}}
\citation{liPersonSearchNatural2017}
\citation{Zhu}
\citation{dingSemanticallySelfAlignedNetwork2021}
\citation{jiangCrossModalImplicitRelation2023}
\citation{liuCausalityInspiredInvariantRepresentation2024}
\citation{zuoUFineBenchTowardsTextbased2024}
\citation{yanPrototypicalPromptingTexttoimage2024}
\citation{qinNoisyCorrespondenceLearningTexttoImage2024}
\citation{jiangModelingThousandsHuman2025}
\citation{zhengCPCLCrossModalPrototypical2024}
\citation{liPromptDecouplingTexttoImage2024}
\citation{dingSemanticallySelfAlignedNetwork2021}
\citation{yanCLIPDrivenFinegrainedTextImage2022}
\citation{shuSeeFinerSeeMore2022}
\newlabel{eq:total}{{6}{5}{}{equation.6}{}}
\newlabel{eq:total@cref}{{[equation][6][]6}{[1][4][]5}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {IV}Experiments}{5}{}\protected@file@percent }
\newlabel{sec:experiments}{{IV}{5}{}{section.4}{}}
\newlabel{sec:experiments@cref}{{[section][4][]IV}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-A}}Implementation Details}{5}{}\protected@file@percent }
\newlabel{sec:4.2}{{\mbox  {IV-A}}{5}{}{subsection.4.1}{}}
\newlabel{sec:4.2@cref}{{[subsection][1][4]\mbox  {IV-A}}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-B}}Parameter and Efficiency Analysis}{5}{}\protected@file@percent }
\newlabel{sec:param_analysis}{{\mbox  {IV-B}}{5}{}{subsection.4.2}{}}
\newlabel{sec:param_analysis@cref}{{[subsection][2][4]\mbox  {IV-B}}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-C}}Ablation Study}{5}{}\protected@file@percent }
\@writefile{lot}{\contentsline {table}{\numberline {I}{\ignorespaces Ablation study on the BDAM module.}}{5}{}\protected@file@percent }
\newlabel{tab:ablation_bdam}{{I}{5}{}{table.1}{}}
\newlabel{tab:ablation_bdam@cref}{{[table][1][]I}{[1][5][]5}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {II}{\ignorespaces Ablation study on the fusion module.}}{5}{}\protected@file@percent }
\newlabel{tab:ablation_fusion}{{II}{5}{}{table.2}{}}
\newlabel{tab:ablation_fusion@cref}{{[table][2][]II}{[1][5][]5}{}{}{}}
\@writefile{lot}{\contentsline {table}{\numberline {III}{\ignorespaces Ablation study on individual loss components.}}{5}{}\protected@file@percent }
\newlabel{tab:ablation_loss}{{III}{5}{}{table.3}{}}
\newlabel{tab:ablation_loss@cref}{{[table][3][]III}{[1][5][]5}{}{}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {\mbox  {IV-D}}Comparisons with State-of-the-Art Methods}{5}{}\protected@file@percent }
\bibstyle{IEEEbib}
\bibdata{icme2025references}
\bibcite{shaoLearningGranularityUnifiedRepresentations2022}{1}
\bibcite{dingSemanticallySelfAlignedNetwork2021}{2}
\bibcite{bukhariLanguageVisionBased2023}{3}
\bibcite{galiyawalaPersonRetrievalSurveillance2021}{4}
\bibcite{wangLearningDeepStructurePreserving2016}{5}
\bibcite{wangLanguagePersonSearch2019}{6}
\bibcite{gaoContextualNonLocalAlignment2021}{7}
\bibcite{wangCAIBCCapturingAllround2022}{8}
\bibcite{wangViTAAVisualTextualAttributes2020}{9}
\bibcite{jiangCrossModalImplicitRelation2023}{10}
\bibcite{dosovitskiyImageWorth16x162021}{11}
\bibcite{tanHarnessingPowerMLLMs2024}{12}
\bibcite{yangUnifiedTextbasedPerson2023}{13}
\bibcite{radfordLearningTransferableVisual2021}{14}
\bibcite{yaoFILIPFinegrainedInteractive2021}{15}
\bibcite{niuLLMLocBootstrapSingleimage2025}{16}
\@writefile{lot}{\contentsline {table}{\numberline {IV}{\ignorespaces Performance comparison with state-of-the-art methods on three benchmark datasets.}}{6}{}\protected@file@percent }
\newlabel{tab:sota_comparison}{{IV}{6}{}{table.4}{}}
\newlabel{tab:sota_comparison@cref}{{[table][4][]IV}{[1][5][]6}{}{}{}}
\@writefile{toc}{\contentsline {section}{\numberline {V}Conclusion and Limitations}{6}{}\protected@file@percent }
\newlabel{sec:conclusion}{{V}{6}{}{section.5}{}}
\newlabel{sec:conclusion@cref}{{[section][5][]V}{[1][6][]6}{}{}{}}
\@writefile{toc}{\contentsline {section}{References}{6}{}\protected@file@percent }
\bibcite{jiangModelingThousandsHuman2025}{17}
\bibcite{wangDisentangledRepresentationLearning2024}{18}
\bibcite{liuMultitaskAdversarialNetwork2018}{19}
\bibcite{chengDisentangledFeatureRepresentation2021}{20}
\bibcite{materzynskaDisentanglingVisualWritten2022}{21}
\bibcite{liDisentanglingIdentityFeatures2024}{22}
\bibcite{azadActivityBiometricsPersonIdentification2024}{23}
\bibcite{schmidtRobustCanonicalizationBootstrapped2025}{24}
\bibcite{yinGraFTGradualFusion2023}{25}
\bibcite{kimExtendingCLIPImageText2024}{26}
\bibcite{fengKnowledgeGuidedDynamicModality2024}{27}
\bibcite{li2019graph}{28}
\bibcite{hanafiFastDBSCANAlgorithm2022}{29}
\bibcite{liPersonSearchNatural2017}{30}
\bibcite{Zhu}{31}
\bibcite{liuCausalityInspiredInvariantRepresentation2024}{32}
\bibcite{zuoUFineBenchTowardsTextbased2024}{33}
\bibcite{yanPrototypicalPromptingTexttoimage2024}{34}
\bibcite{qinNoisyCorrespondenceLearningTexttoImage2024}{35}
\bibcite{zhengCPCLCrossModalPrototypical2024}{36}
\bibcite{liPromptDecouplingTexttoImage2024}{37}
\bibcite{yanCLIPDrivenFinegrainedTextImage2022}{38}
\bibcite{shuSeeFinerSeeMore2022}{39}
\gdef \@abspage@last{7}
