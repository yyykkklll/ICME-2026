\documentclass[conference]{IEEEtran}
\pdfoutput=1
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Disentangling Identity from Transient Attributes: A Semantically-Supervised Decoupling Framework for Robust Person Re-Identification}

\author{Anonymous ICME submission}

\maketitle

\begin{abstract}
    Text-to-Image Person Re-Identification faces significant challenges from clothing-induced interference and persistent modality gaps. 
    To address this, we propose a novel and robust person re-identification framework. 
    By leveraging a Multimodal Large Language Model (MLLM), it effectively decouples person identity from other interference features, thereby achieving accurate person identification.
    Specifically, we introduce a Bidirectional Decoupled Alignment Module (BDAM). 
    By employing MLLM-generated descriptions as fine-grained semantic supervision, it explicitly disentangles visual features into distinct identity and clothing subspaces. 
    This decoupling strategy is enforced by a multi-objective regularization term combining clothing alignment loss with kernel-based orthogonal constraints.
    Furthermore, our network is based on Mamba, which efficiently models long-range dependencies within image content, avoiding the quadratic costs associated with Transformers.
    Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid benchmarks demonstrate the effectiveness of our method, showing superior performance and robustness against clothing variations compared to leading contemporary approaches.
\end{abstract}

\begin{IEEEkeywords}
    Multimodal Learning, Text-to-Image Re-Identification, Feature Decoupling, Semantic Supervision.
\end{IEEEkeywords}

\section{Introduction}
Text-to-Image Person Re-Identification (T2I-ReID) retrieves a target pedestrian from a large-scale image gallery given a natural-language description~\cite{dingSemanticallySelfAlignedNetwork2021}.
This technology is valuable for video surveillance~\cite{bukhariLanguageVisionBased2023}, intelligent security~\cite{galiyawalaPersonRetrievalSurveillance2021}, public safety, and social media.
Despite recent progress, practical deployment remains challenging due to image factors (e.g., pose, viewpoint, illumination) obscuring identity cues and a persistent modality gap hampering fusion\cite{daiDiffusionbasedSyntheticData2025}.
These issues are exacerbated at the fine-grained level, where semantic alignment is particularly difficult\cite{zuoUFineBenchTowardsTextbased2024}.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure1}
    \vspace{-4mm}
    \caption{Comparison of ReID methods. (a) Entangled global feature fusion. (b) \textit{Our method}: MLLM-guided decoupling of identity and transient attributes (e.g., clothing and accessories).}
    \vspace{-4mm}
    \label{fig:figure1}
\end{figure}

Due to noisy environments and human morphological variations, a core challenge in T2I-ReID is the substantial semantic gap between images and text. 
Early work projected global visual and textual features into a shared space~\cite{wangLearningDeepStructurePreserving2016}, but suffered from high intra-class and low inter-class variance. 
To mitigate this, subsequent studies adopted feature disentanglement to align latent semantics, broadly falling into two categories: explicit methods using auxiliary modules for part-level alignment~\cite{wangViTAAVisualTextualAttributes2020}, and implicit approaches leveraging regularizers to associate noun phrases with image regions~\cite{jiangCrossModalImplicitRelation2023,zhangMultiStageAuxiliaryLearning2024}. 
This evolution underscores the importance of separating identity-relevant from irrelevant semantics for advancing T2I-ReID.

This pursuit of disentanglement has been propelled by powerful backbones. 
For instance, models employing ViT~\cite{dosovitskiy2020image} capture fine-grained details, while methods leveraging CLIP~\cite{radford2021learning} learn a well-aligned joint space.
However, CLIP-based methods typically rely on global image-text alignment. 
This holistic approach often fails to distinguish identity-stable features (e.g., body shape) from transient attributes (e.g., clothing, accessories, and belongings; hereafter collectively referred to as ``clothing" for brevity), leading to entangled representations where salient transient details overshadow stable identity cues.
In contrast, as illustrated in Fig.~\ref{fig:figure1}, our framework demonstrates that precise, semantic-aware supervision is more critical than raw pre-training scale.
Despite these advances, a critical limitation persists: both lines of work commonly treat the textual description holistically.
This overlooks the semantic distinction between content relevant to identity and content irrelevant to identity.
This coarse-grained treatment forces the model to entangle these factors, often prioritizing salient transient details over stable identity cues, which blurs identity features and degrades matching robustness.
To address this challenge, we propose a novel framework with explicit, fine-grained semantic supervision. 
However, a primary obstacle is that existing datasets lack specific annotations separating identity attributes from transient details. 
To bridge this gap, we introduce a Style-Guided Semantic Enhancement strategy.

\section{Related Work}
\label{sec:related_work}
Feature disentanglement~\cite{wangDisentangledRepresentationLearning2024} aims to separate semantically distinct factors to improve generalization. 
Early work used generative models~\cite{liuMultitaskAdversarialNetwork2018}, while modern strategies employ adversarial training~\cite{liuMultitaskAdversarialNetwork2018}, metric learning~\cite{chengDisentangledFeatureRepresentation2021}, or orthogonal projections~\cite{materzynskaDisentanglingVisualWritten2022}. 
In the ReID domain, this is used to separate identity-relevant signals from nuisances~\cite{liDisentanglingIdentityFeatures2024}, such as occlusion~\cite{cuiProFDPromptGuidedFeature2024} or clothing changes~\cite{liDisentanglingIdentityFeatures2024}.

While these efforts improve semantic purity, critical limitations persist. 
Without an effective interaction mechanism, isolated factors may fail to support robust cross-modal matching~\cite{azadActivityBiometricsPersonIdentification2024}. 
Reliance on manual annotations or external detectors constrains scalability~\cite{cuiProFDPromptGuidedFeature2024}, and implicit regularizers can be underconstrained, yielding spurious separations on unseen data~\cite{jiangCrossModalImplicitRelation2023}. 
Consequently, recent work emphasizes coupling disentanglement with principled interaction and independence constraints~\cite{azadActivityBiometricsPersonIdentification2024,luARGUSDefendingMultimodal2025}. 
Our framework addresses this by pairing explicit supervision with model-level disentanglement to preserve cross-modal synergy.

\section{METHODOLOGY}
\label{sec:method}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{Figure2.pdf}
    \vspace{-4mm}
    \caption{Overview of the proposed framework. An MLLM generates identity ($f^t_{\mathrm{id}}$) and clothing ($f^t_{\mathrm{clo}}$) descriptions to supervise the BDAM. The BDAM module disentangles the input visual feature $f_i$ into identity ($f_{\mathrm{id}}$) and clothing ($f_{\mathrm{clo}}$) features, which are optimized via contrastive and decoupling losses. Finally, the fusion module integrates the visual and textual identity features ($f_{\mathrm{id}}$ and $f_{\mathrm{id}}^t$).}
    \vspace{-4mm}
    \label{fig:figure2}
\end{figure*}

Drawing inspiration from the style-clustering paradigm~\cite{jiangModelingThousandsHuman2025}, this strategy leverages an MLLM to automatically generate distinct descriptions for identity and clothing.
These decoupled annotations provide precise supervision for our Bidirectional Decoupled Alignment Module (BDAM) to meticulously separate and align identity and clothing information.
This separation is enforced by a multi-objective loss strategy, including an alignment loss and an HSIC-based orthogonality constraint\cite{gretton2005measuring}.
Furthermore, we pioneer the integration of the Mamba as an efficient fusion module\cite{gu2024mamba}, adept at capturing long-range cross-modal dependencies with linear complexity.

Our main contributions are summarized as follows:
\begin{itemize}
    \item We introduce a Style-Guided Semantic Enhancement strategy that leverages clustering and MLLMs to generate fine-grained, decoupled identity and clothing descriptions, providing precise supervision for explicit feature disentanglement.
    \item We design the Bidirectional Decoupled Alignment Module (BDAM), which achieves precise decoupling and alignment reinforced by a multi-objective loss strategy combining an alignment loss and an orthogonality constraint based on HSIC.
    \item We empirically validate the proposed Mamba-based fusion strategy through comprehensive experiments, which demonstrate its capability to efficiently model cross-modal dependencies and outperform contemporary methods in handling clothing interference.
\end{itemize}

\subsection{Overview}
To learn robust pedestrian representations, we propose the BDAM, which disentangles features using textual guidance.
As illustrated in Fig.~\ref{fig:figure2}, our framework comprises vision and text encoders, the core BDAM, and an efficient Mamba Fusion Module.
Given an image $I \in \mathbb{R}^{B \times C \times H \times W}$, a visual encoder extracts features $f_i$.
Concurrently, we use an MLLM to generate separate identity and clothing descriptions, which a text encoder encodes into $f^t_{\mathrm{id}}$ and $f^t_{\mathrm{clo}}$.
The BDAM leverages these textual features to guide the disentanglement of $f_i$.
To enforce this separation, we employ two losses: an alignment loss ($\mathcal{L}_{\mathrm{aln}}$) to supervise the visual clothing features using clothing descriptions, and an HSIC-based loss to ensure orthogonality.
Finally, the Mamba efficiently fuses the disentangled visual identity and textual semantics, enhancing the model's overall representation.

\subsection{Semantic Enhancement}
\label{sec:semantic}
\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure3.pdf}
    \vspace{-4mm}
    \caption{Overview of the offline pipeline using an MLLM to generate decoupled identity and clothing descriptions. Style prompts are derived via CLIP and DBSCAN to enhance diversity.}
    \vspace{-4mm}
    \label{fig:mllm_pipeline}
\end{figure}

We employ an MLLM (GPT-4) to automatically generate fine-grained identity and clothing descriptions for pedestrians, reducing the burden of manual annotation and enriching the available supervision. 
Fig.~\ref{fig:mllm_pipeline} illustrates this generation pipeline. Inspired by prior work on modeling annotator styles~\cite{jiangModelingThousandsHuman2025}, we first derive style categories to guide the MLLM's generation tone.

To achieve this, we use the CLIP text encoder to embed original descriptions and then cluster these style embeddings with DBSCAN. 
These discovered style categories are used to formulate textual prompts (e.g., ``Use a very detailed, descriptive style"). 
A dual prompt generator, using content-specific templates (e.g., ``Describe the person's identity" and ``Describe the person's clothing"), then guides the MLLM to output two distinct texts per image: one description for identity (biological traits) and another for clothing (apparel, colors, and patterns). 
We apply syntax checks and validation to ensure the outputs remain grammatical and structured.

\subsection{Bidirectional Decoupled Alignment Module}
CLIP-based global alignment suffers from two limitations: insufficient fine-grained semantics for separating identity from clothing, and lack of token-level cross-modal interaction, which degrades robustness in complex scenes.

We adopt a pre-trained ViT (ViT-B/16) as the visual encoder $E_v$, producing entangled patch tokens $f_i \in \mathbb{R}^{B \times L \times D}$. These are projected into two branches yielding preliminary identity ($f_{\text{id}}'$) and clothing ($f_{\text{clo}}'$) features, which are refined via stream-wise self-attention. Unlike CLIP, we utilize the full patch sequence and introduce cross-attention between the two streams to enhance semantic distinction.

Each stream is globally averaged to obtain $\hat{f}_\text{id}$ and $\hat{f}_\text{clo}$. A lightweight gating network—comprising concatenation, a linear layer, and a Sigmoid—produces a soft mask $g \in \mathbb{R}^{B \times D}$. The final disentangled features are:
\[
f_{\mathrm{id}}^{i} = g \odot \hat{f}_{\mathrm{id}}, \quad f_{\mathrm{clo}}^{i} = (1 - g) \odot \hat{f}_{\mathrm{clo}},
\]
where $\odot$ denotes element-wise multiplication. Only $f_{\mathrm{id}}^{i}$ is forwarded to the fusion module.

Training is guided by two losses. First, a clothing alignment loss $\mathcal{L}_{\mathrm{aln}}$ aligns visual clothing features with MLLM-generated textual descriptions:
\begin{equation} \label{eq:aln}
    \mathcal{L}_{\mathrm{aln}} = -\mathbb{E}_i \left[ \log \frac{\exp(s_{ii} / \tau)}{\sum_j \exp(s_{ij} / \tau)} \right],
\end{equation}
where $s_{ij} = \hat{f}_{\mathrm{clo}}^i \cdot (f_{\mathrm{clo}}^t)^j$ is the dot-product similarity (after L2 normalization and dimension alignment), and $\tau$ is a temperature. This explicitly defines clothing semantics, indirectly purifying the identity stream.

Second, to enforce statistical independence, we minimize an HSIC-based decoupling loss:
\begin{IEEEeqnarray}{rCl}
    \mathcal{L}_{\mathrm{Decouple}} & = & \mathrm{HSIC}(f_{\mathrm{id}}^{i}, f_{\mathrm{clo}}^{i}) \nonumber \\
    & = & \frac{1}{(N-1)^2} \mathrm{tr}(K_{\mathrm{id}} H K_{\mathrm{clo}} H),
    \label{eq:decouple}
\end{IEEEeqnarray}
where $K_{\mathrm{id}} = f_{\mathrm{id}}^{i} (f_{\mathrm{id}}^{i})^\top$, $K_{\mathrm{clo}} = f_{\mathrm{clo}}^{i} (f_{\mathrm{clo}}^{i})^\top$, and $H = I_N - \frac{1}{N} \mathbf{1}_N \mathbf{1}_N^\top$ is the centering matrix. Minimizing HSIC encourages orthogonality between identity and clothing subspaces.

\subsection{Feature Fusion}
We introduce the Mamba SSM for efficient, semantic-sensitive feature fusion. 
The core objective is to preserve the semantic integrity of the purified identity features from both image and text, enabling a fusion that is robust to clothing variations isolated by the BDAM. 
As illustrated in Fig.~\ref{fig:fusion_module}, the process begins with an FFN performing dimensional alignment on the decoupled visual features $f_{\text{id}}^{\text{i}}$ and the textual features $f_{\text{id}}^{t}$ to generate $f_{\text{img}}$ and $f_{\text{txt}}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Figure4.pdf}
    \vspace{-4mm}
    \caption{Architecture of the Mamba Fusion Module, which fuses visual ($f_{\text{id}}^{\text{i}}$) and textual ($f_{\text{id}}^{\text{t}}$) identity features while discarding clothing features ($f_{\text{clo}}^{\text{i}}$).}
    \vspace{-4mm}
    \label{fig:fusion_module}
\end{figure}

Notably, the decoupled visual clothing feature $f_{\text{clo}}^{\text{i}}$ is \textit{intentionally discarded} during fusion. This design is central to our goal: since the BDAM (supervised by $\mathcal{L}_{\text{aln}}$ and $\mathcal{L}_{\text{Decouple}}$) is tasked with purging irrelevant information into $f_{\text{clo}}^{\text{i}}$, excluding this feature forces the model to learn a representation based purely on stable identity semantics.

Following this alignment, a gating mechanism achieves dynamic weighted fusion. 
It outputs a weight vector $g_{\text{fus}} \in \mathbb{R}^{B \times 2}$, which is normalized via a SoftMax layer to produce image $W_{img}$ and text $W_{txt}$ weights ($W_{img} + W_{txt} = 1$). 
The resulting fusion is computed as: $f_{\text{fusion}} = W_{img} \cdot f_{\text{img}} + W_{txt} \cdot f_{\text{txt}}$.

The resulting $f_{\text{fusion}}$ features are then processed by the Mamba SSM to enhance cross-modal interaction. 
Leveraging its capability to model long-range dependencies, Mamba effectively captures complex sequential relationships. 
We employ a stack of Mamba layers, where each layer updates its input $f_{fusion}^{(l)}$ using a residual connection: $f_{fusion}^{(l+1)} = \text{Mamba}(f_{fusion}^{(l)}) + f_{fusion}^{(l)}$. 
This structure improves information flow. 
Finally, the output from the last Mamba layer is projected to produce the final representation, $f_{final} \in \mathbb{R}^{B \times D_{out}}$.

\subsection{Loss Function}
To achieve fine-grained alignment between modalities, we adopt the InfoNCE loss ($\mathcal{L}_{\mathrm{info}}$), which maximizes similarity for positive image-text pairs while separating negatives:
\begin{equation}
    \label{eq:info}
    \mathcal{L}_{\mathrm{info}} = -\log \frac{\exp(v_i^{\top} t_i / \tau)}{\sum_j \exp(v_i^{\top} t_j / \tau)}
\end{equation}
Here, $v_i$ is the final L2-normalized fused representation, $t_i$ is the text feature for the $i$-th identity, and $\tau$ controls distribution sharpness.

To enhance intra-modality identity discrimination, we include a triplet loss ($\mathcal{L}_{\mathrm{triplet}}$) to enforce class compactness and separation:
\begin{equation}
    \label{eq:triplet}
    \mathcal{L}_{\mathrm{triplet}} = \mathbb{E}_{(a,p,n)} \left[ \max\left( \left\| f_a - f_p \right\|_2^2 - \left\| f_a - f_n \right\|_2^2 + m, 0 \right) \right]
\end{equation}
Here, $f_a,f_p,f_n$ are the decoupled visual identity features for the anchor, positive, and negative samples, respectively, and $m$ is the margin.

In training with multiple tasks, differing loss scales can cause one task to dominate. 
We adopt GradNorm to balance the training process by dynamically adjusting the gradient norm of each task:
\begin{equation} \label{eq:gradnorm}
    \mathcal{L}_{\mathrm{GradNorm}} = \sum_k \left| \nabla_{\theta} (w_k \mathcal{L}_k) - \tilde{r}_k G_{\mathrm{ref}} \right|
\end{equation}
Here, $\nabla_{\theta} (w_k \mathcal{L}_k)$ is the gradient of the weighted loss of task $k$ w.r.t. shared parameters $\theta$. 
$\tilde{r}_k = (\mathcal{L}_k / \mathcal{L}_k^0) / \bar{r}$ is the normalized loss ratio, where $\mathcal{L}_k^0$ is the initial loss, and $G_{\mathrm{ref}}$ is a reference gradient norm.

Finally, the overall loss function is defined as follows, including a regularization term to prevent instability:
\begin{equation}
    \label{eq:total}
    \mathcal{L}_{\mathrm{Total}} = \sum_k w_k \mathcal{L}_k + \alpha \mathcal{L}_{\mathrm{GradNorm}} + \lambda \sum_k (\log w_k)^2
\end{equation}
Here, $\mathcal{L}_k$ represents the loss term for task $k$ (which includes $\mathcal{L}_{\mathrm{info}}$, $\mathcal{L}_{\mathrm{triplet}}$, $\mathcal{L}_{\mathrm{aln}}$, and $\mathcal{L}_{\mathrm{Decouple}}$), and $w_k = \exp(s_k)$ is the learnable task weight. 
The hyperparameters $\alpha$ and $\lambda$ control the GradNorm strength and regularization, respectively.
In Table~IV, \emph{w/o Weight Reg.} denotes removing the regularizer $\lambda \sum_k (\log w_k)^2$ in Eq.~\ref{eq:total}.

\section{Experiments}
\label{sec:experiments}
\begin{table*}[t]
    \centering
    \caption{Performance comparison with state-of-the-art methods on three benchmarks.
    ``Ours" corresponds to the full model with BDAM, Mamba fusion, and all loss components enabled.
    Unless otherwise specified, the same configuration is used in all ablation and robustness studies.}

    \label{tab:sota_comparison}
    \vspace{-0.3cm}
    \resizebox{\textwidth}{!}{
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{l|c|cccc|cccc|cccc}
            \hline
            \multirow{2}{*}{\textbf{Method}}
                                                                        & \multirow{2}{*}{\textbf{Backbone}}
                                                                        & \multicolumn{4}{c|}{\textbf{CUHK-PEDES}}
                                                                        & \multicolumn{4}{c|}{\textbf{ICFG-PEDES}}
                                                                        & \multicolumn{4}{c}{\textbf{RSTPReid}}                                                                                                                              \\
            \cline{3-6} \cline{7-10} \cline{11-14}
                                                                        &                                          & \textbf{R-1}   & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}                                                                          \\
            \hline
            \multicolumn{14}{l}{\quad\small\textit{Methods with CLIP backbone:}}                                                                                                                                                             \\
            \hline
            IRRA~\cite{jiangCrossModalImplicitRelation2023}             & CLIP-ViT
                                                                        & 73.38                                    & 89.93          & 93.71          & 66.10          & 63.36        & 80.82 & 85.82 & 38.06 & 60.20 & 81.30 & 88.20 & 47.17 \\
            IRLT~\cite{liuCausalityInspiredInvariantRepresentation2024} & CLIP-ViT
                                                                        & 73.67                                    & 89.71          & 93.57          & 65.94          & 63.57        & 80.57 & 86.32 & 38.34 & 60.51 & 82.85 & 89.71 & 47.64 \\
            CFAM~\cite{zuoUFineBenchTowardsTextbased2024}               & CLIP-ViT
                                                                        & 74.46                                    & 90.19          & 94.01          & -              & 64.72        & 81.35 & 86.31 & -     & 61.49 & 82.26 & 89.23 & -     \\
            Propot~\cite{yanPrototypicalPromptingTexttoimage2024}       & CLIP-ViT
                                                                        & 74.89                                    & 89.90          & 94.17          & 67.12          & 65.12        & 81.57 & 86.97 & 42.93 & 61.87 & 83.63 & 89.70 & 47.82 \\
            RDE~\cite{qinNoisyCorrespondenceLearningTexttoImage2024}    & CLIP-ViT
                                                                        & 75.94                                    & 90.14          & 94.12          & 67.56          & 67.68        & 82.47 & 87.36 & 40.06 & 65.35 & 83.95 & 89.90 & 50.88 \\
            HAM~\cite{jiangModelingThousandsHuman2025}                  & CLIP-ViT
                                                                        & 77.99                                    & 91.34          & 95.03          & 69.72          & 69.95        & 83.88 & 88.39 & 42.72 & 72.50 & 87.70 & 91.95 & 55.47 \\
            \hline
            \multicolumn{14}{l}{\quad\small\textit{Methods with ViT backbone:}}                                                                                                                                                              \\
            \hline
            CPCL~\cite{zhengCPCLCrossModalPrototypical2024}             & ViT
                                                                        & 70.03                                    & 87.28          & 91.78          & 63.19          & 62.60        & 79.07 & 84.46 & 36.16 & 58.35 & 81.05 & 87.65 & 45.81 \\
            PDReid~\cite{liPromptDecouplingTexttoImage2024}             & ViT
                                                                        & 71.59                                    & 87.95          & 92.45          & 65.03          & 60.93        & 77.96 & 84.11 & 36.44 & 56.65 & 77.40 & 84.70 & 45.27 \\
            SSAN~\cite{dingSemanticallySelfAlignedNetwork2021}          & ViT
                                                                        & 61.37                                    & 80.15          & 86.73          & -              & 54.23        & 72.63 & 79.53 & -     & 43.50 & 67.80 & 77.15 & -     \\
            CFine~\cite{yanCLIPDrivenFinegrainedTextImage2022}          & ViT
                                                                        & 69.57                                    & 85.93          & 91.15          & -              & 60.83        & 76.55 & 82.42 & -     & 50.55 & 72.50 & 81.60 & -     \\
            IVT~\cite{shuSeeFinerSeeMore2022}                           & ViT
                                                                        & 65.59                                    & 83.11          & 89.21          & -              & 56.04        & 73.60 & 80.22 & -     & 46.70 & 70.00 & 78.80 & -     \\
            \hline
            \rowcolor{gray!20}
            \textbf{Ours}                                               & ViT
                                                                        & \textbf{79.93}                           & \textbf{91.93} & \textbf{96.47} & \textbf{72.61}
                                                                        & \textbf{68.68}                           & \textbf{84.29} & \textbf{89.74} & \textbf{41.78}
                                                                        & \textbf{74.33}                           & \textbf{88.85} & \textbf{92.95} & \textbf{57.68}                                                                        \\
            \hline
        \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}

\subsection{Implementation Details}
\label{sec:implementation_details}
\subsubsection{Datasets and Metrics}
\label{sec:datasets_metrics}
We evaluate on three benchmarks: CUHK-PEDES~\cite{li2017person}, ICFG-PEDES~\cite{zhuDSSLDeepSurroundingsperson2021}, and RSTPReid~\cite{dingSemanticallySelfAlignedNetwork2021}, adhering to their official identity-based splits. 
Performance is reported using mean Average Precision (mAP) and Rank-$k$ accuracy ($k=1, 5, 10$).

\subsubsection{Model and Training}
\label{sec:model_training}
We employ pre-trained \textit{bert-base-uncased} and \textit{vit-base-patch16-224} as text and visual encoders, respectively, with $224 \times 224$ inputs. 
BDAM outputs 768-dimensional features, which are integrated by a 2-layer Mamba fusion module (dim 256, state 16, kernel 4) with 0.1 dropout. 
Optimization uses Adam ($LR=1 \times 10^{-4}$, $WD=1 \times 10^{-3}$) with cosine annealing. 
The total loss (InfoNCE, triplet, alignment, HSIC) is dynamically balanced via GradNorm ($\alpha = 1.5$).

\subsubsection{Data Augmentation and Reporting}
\label{sec:data_augmentation}
We generate decoupled descriptions using ChatGPT-4, guided by style clusters derived from CLIP and DBSCAN. 
All experiments are repeated across three random seeds (0, 1, 2) to report mean results.

\subsection{Parameter Analysis}
\label{sec:param_analysis}
\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{gate_weight_distribution.pdf}
    \caption{Learned gate weight distributions on CUHK-PEDES. (a) BDAM's identity-centric bias and (b) Fusion's balanced modality contributions.}
    \vspace{-4mm}
    \label{fig:gate_distribution}
\end{figure}

We empirically validate our gating mechanisms by analyzing the learned weights in Fig.~\ref{fig:gate_distribution}.
Driven by asymmetric losses, the BDAM's dimension-level gate (($g_{\text{dis}} \equiv g$)) exhibits a clear identity-centric bias, with identity weights ($\mu_{\mathrm{id}}=0.61$) significantly exceeding clothing weights ($\mu_{\mathrm{clo}}=0.39$).
In contrast, the instance-level fusion gate ($g_{\text{fus}}$) maintains equilibrium between modalities, with comparable image and text contributions ($\mu_{\mathrm{img}}=0.52$ vs. $\mu_{\mathrm{txt}}=0.48$), preventing modality collapse.
These statistics corroborate our design: BDAM enforces semantic disentanglement, while the fusion module achieves dynamic cross-modal balance.

\subsection{Ablation Study}
We conduct systematic ablation studies on CUHK-PEDES to validate each component.

\begin{table}[t]
    \centering
    \caption{Ablation study on the BDAM module.}
    \label{tab:ablation_bdam}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                  & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o BDAM)     & 59.81              & 70.54              & 85.49              & 91.26               \\
        + BDAM                  & 66.74              & 76.27              & 89.30              & 94.02               \\
        \quad w/o Cross-Attn    & 62.56              & 71.39              & 87.05              & 92.98               \\
        \quad w/o Gate          & 65.11              & 74.63              & 88.77              & 93.56               \\
        \quad Shallow (3-layer) & 64.27              & 73.74              & 88.09              & 93.32               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}
%
\noindent \textbf{Disentanglement module.} Table~\ref{tab:ablation_bdam} shows that BDAM yields substantial gains.
Removing cross-attention, ablating the gating mechanism, or reducing depth all degrade performance, confirming that semantic interaction and adaptive control are essential for robust disentanglement.

\begin{table}[t]
    \centering
    \caption{Ablation study on the fusion module.}
    \label{tab:ablation_fusion}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{3.5pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o Fusion) & 59.81              & 70.54              & 85.49              & 91.26               \\
        Full Fusion           & 72.61              & 79.93              & 90.74              & 95.11               \\
        \quad w/o Mamba       & 66.89              & 75.73              & 89.06              & 93.92               \\
        \quad w/o Gate        & 68.64              & 77.58              & 90.11              & 94.87               \\
        \quad w/o Alignment   & 68.15              & 77.09              & 89.84              & 94.53               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Fusion module.} Table~\ref{tab:ablation_fusion} confirms the synergy of our fusion components. 
The Mamba SSM is critical; its removal causes the largest drop, highlighting the value of long-range dependency modeling. The gating and alignment layers provide further essential gains.

\begin{table}[t]
    \centering
    \caption{Ablation study on individual loss components.}
    \label{tab:ablation_loss}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method         & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Full Model     & 72.61              & 79.93              & 91.93              & 96.47               \\
        w/o InfoNCE    & 28.14              & 36.55              & 55.21              & 65.83               \\
        w/o Triplet    & 67.22              & 74.89              & 88.15              & 93.12               \\
        w/o Alignment  & 69.15              & 76.92              & 89.53              & 94.22               \\
        w/o Decoupling & 70.03              & 77.81              & 90.11              & 94.98               \\
        w/o Weight Reg.  & 71.98              & 79.23              & 91.35              & 95.71               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\noindent \textbf{Loss and prompting.} Table~\ref{tab:ablation_loss} validates our multi-objective loss. 
InfoNCE and triplet losses are foundational. 
Crucially, the degradation without alignment or HSIC decoupling losses proves that explicit constraints are necessary to enforce identity-clothing separation. 
For prompt generation, we found density-based clustering (DBSCAN) significantly outperforms K-Me and random sampling, as it adaptively handles irregular style distributions. 
Full clustering results and t-SNE visualizations are in the supplementary material.

\subsection{Robustness Analysis}
\label{sec:robustness}

To rigorously evaluate the efficacy of the proposed decoupling strategy—specifically the architectural decision to structurally discard visual clothing features during fusion—we conducted a robustness analysis focusing on semantic interference. 
We devised three distinct inference configurations on the CUHK-PEDES dataset based on the MLLM-generated annotations. 
The first configuration, denoted as \textbf{Standard}, utilizes the original, correct identity and clothing descriptions to establish the performance upper bound. 
The second, \textbf{Identity-Only}, masks all clothing-related descriptions in the textual input to assess the model's dependency on transient attributes. 
The third and most challenging configuration is the \textbf{Clothes-Conflict} attack, where we retain correct identity descriptions but shuffle the clothing descriptions across the entire test set. 
This creates a severe semantic mismatch where the input text describes an outfit completely contradictory to the target image.

\begin{table}[t]
    \centering
    \caption{Robustness analysis against semantic interference on CUHK-PEDES.}
    \label{tab:robustness}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Setting          & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Standard         & 72.61              & 79.93              & 91.93              & 96.47               \\
        Identity-Only    & 70.92              & 78.45              & 91.80              & 95.88               \\
        Clothes-Conflict & 68.24              & 76.12              & 89.65              & 94.30               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

The quantitative results are presented in Table~\ref{tab:robustness}. 
In the Identity-Only setting, the model exhibits only a marginal performance decline compared to the Standard setting. This stability corroborates that the BDAM successfully isolates sufficient identity-related semantics into the identity subspace, allowing accurate retrieval without reliance on clothing context. 
More notably, in the Clothes-Conflict scenario, where conventional global-alignment methods typically suffer from drastic degradation due to misaligned semantics, our method maintains a highly competitive accuracy. 
This robustness validates our core design: by enforcing statistical independence via the HSIC loss and intentionally discarding the decoupled visual clothing feature ($f_{clo}^i$) within the Mamba fusion module, the framework effectively shields the identity retrieval process from misleading semantic noise.

\subsection{Comparisons with State-of-the-Art Methods}
Table~\ref{tab:sota_comparison} shows that our method establishes new benchmarks on CUHK-PEDES and RSTPReid while remaining highly competitive on ICFG-PEDES. 
Notably, our ViT-based framework outperforms strong CLIP-based competitors (e.g., HAM, IRRA), challenging the dominance of large-scale pre-training in T2I-ReID. 
We attribute this success to two factors:

\noindent \textbf{Explicit Decoupling vs. Implicit Alignment.} 
Unlike CLIP which implicitly learns potential correlations, our BDAM mathematically enforces the separation of clothing from identity, effectively preventing the network from overfitting to clothing shortcuts.

\noindent \textbf{MLLM as a Teacher.} 
The fine-grained descriptions generated by the MLLM provide superior semantic targets compared to the noisy, coarse-grained alignment of CLIP. 
This validates that task-specific structural design combined with high-quality semantic guidance can effectively surpass the benefits of massive generic pre-training.

\section{Conclusion and Limitations}
\label{sec:conclusion}
We proposed a framework to address clothing interference in T2I-ReID by using MLLM to guide feature decoupling. 
Our BDAM leverages MLLM-generated descriptions to isolate identity from clothing features, enforced by an alignment and kernel-based orthogonal loss. 
A Mamba SSM provides efficient modality fusion. 
This method achieved new state-of-the-art results on CUHK-PEDES and RSTPReid. 
Limitations include potential MLLM-generated noise and large-scale deployment overhead. 

\bibliographystyle{IEEEbib}
\bibliography{icme2025references}

\end{document}