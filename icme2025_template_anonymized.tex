\documentclass[conference]{IEEEtran}
\pdfoutput=1
\IEEEoverridecommandlockouts
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{algorithmic}
\usepackage{graphicx}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{colortbl}
\usepackage{multirow}
\usepackage{hhline}
\usepackage{booktabs}
\usepackage{float}

\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em
    T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}
\begin{document}

\title{Disentangling Identity from Clothing: A Semantically-Supervised Decoupling Framework for Robust Person Re-Identification}

\author{Anonymous ICME submission}

\maketitle

\begin{abstract}
    Text-to-Image Person Re-Identification faces significant challenges from clothing-induced interference and persistent modality gaps. 
    To address this, we propose a novel framework centered on feature decoupling guided by a Multimodal Large Language Model (MLLM). 
    Specifically, we introduce a Bidirectional Decoupled Alignment Module (BDAM) that leverages MLLM-generated descriptions to explicitly disentangle visual features into distinct identity and clothing subspaces. 
    This separation is enforced by a multi-task strategy combining clothing alignment loss with kernel-based orthogonal constraints. 
    Furthermore, we pioneer the integration of the Mamba State Space Model for cross-modal fusion. 
    Leveraging its linear-time complexity, Mamba models long-range dependencies efficiently, avoiding the quadratic costs of Transformers. 
    Extensive experiments on CUHK-PEDES, ICFG-PEDES, and RSTPReid benchmarks demonstrate the effectiveness of our method, showing superior performance and robustness against clothing variations compared to leading contemporary approaches.
\end{abstract}
\begin{IEEEkeywords}
    Multimodal Learning, Text-to-Image Re-Identification, Feature Decoupling, Semantic Supervision.
\end{IEEEkeywords}

\section{Introduction}
Text-to-Image Person Re-Identification (T2I-ReID) retrieves a target pedestrian from a large-scale image gallery given a natural-language description~\cite{dingSemanticallySelfAlignedNetwork2021}.
It is valuable for video surveillance~\cite{bukhariLanguageVisionBased2023}, intelligent security~\cite{galiyawalaPersonRetrievalSurveillance2021}, public safety, and social media.
Despite recent progress, practical deployment remains challenging due to image factors (pose, viewpoint, illumination) obscuring identity cues and a persistent modality gap hampering fusion.
These issues are exacerbated at the fine-grained level, where semantic alignment is particularly difficult.

\begin{figure}[t]
    \centering
    \includegraphics[width=\columnwidth]{Figure1}
    \caption{Comparison of ReID methods. (a) \textit{Traditional}: Entangled global feature fusion. (b) \textit{Proposed (BDAM)}: MLLM-guided decoupling of identity and clothing.}
    \label{fig:figure1}
\end{figure}

A core challenge in T2I-ReID is the semantic gap between images and text.
Early work attempted to reduce this discrepancy by projecting global visual and textual features into a shared space~\cite{wangLearningDeepStructurePreserving2016}, but struggled with high intra-class and low inter-class variance.
To overcome this, subsequent studies introduced feature disentanglement~\cite{gaoContextualNonLocalAlignment2021}.
These are broadly explicit, using auxiliary modules for part-alignment~\cite{wangViTAAVisualTextualAttributes2020}, or implicit, using regularizers to associate noun phrases with regions~\cite{jiangCrossModalImplicitRelation2023}.
This progression highlights that distinguishing identity-relevant from irrelevant semantics is essential for advancing T2I-ReID.

This pursuit of disentanglement has been propelled by powerful backbones. 
Models employing ViT~\cite{dosovitskiyImageWorth16x162021} capture fine-grained details, while methods leveraging CLIP~\cite{yaoFILIPFinegrainedInteractive2021} learn a well-aligned joint space.
However, CLIP-based methods typically rely on global image-text alignment. 
This holistic approach often fails to distinguish identity-stable features (e.g., body shape) from transient nuisances (e.g., clothing), leading to entangled representations where salient clothing details overshadow stable identity cues.
In contrast, our framework demonstrates that precise, semantic-aware supervision is more critical than raw pre-training scale.
Despite these advances, a critical limitation persists: both lines of work commonly treat the textual description holistically.
This overlooks the semantic distinction between content relevant to identity and content irrelevant to identity.
This coarse-grained treatment forces the model to entangle these factors, often prioritizing salient clothing details over stable identity cues, which blurs identity features and degrades matching robustness.

To address this challenge, we propose a novel framework with explicit, fine-grained semantic supervision.
Inspired by the style-clustering paradigm~\cite{jiangModelingThousandsHuman2025}, our approach employs MLLM to guide feature decoupling by automatically generating distinct descriptions for identity and clothing.
These decoupled annotations provide precise supervision for our Bidirectional Decoupled Alignment Module (BDAM) to meticulously separate and align identity and clothing information.
This separation is enforced by a multi-task loss strategy, including an alignment loss and an HSIC-based orthogonality constraint.
Furthermore, we pioneer the integration of the Mamba state space model (SSM) as an efficient fusion module, adept at capturing long-range cross-modal dependencies with linear complexity.

Our main contributions are summarized as follows:
\begin{itemize}
    \item We propose an automatic prompt construction pipeline that combines style clustering with an MLLM to produce fine-grained, decoupled identity and clothing descriptions.
    \item We design the Bidirectional Decoupled Alignment Module (BDAM), which achieves precise decoupling and alignment reinforced by a multi-task loss strategy combining an alignment loss and an orthogonality constraint based on HSIC.
    \item We introduce the novel integration of a Mamba SSM fusion module that models long-range cross-modal dependencies with linear complexity.
\end{itemize}

\section{Related Work}
\label{sec:related_work}
\subsection{Feature Disentanglement}
Feature disentanglement~\cite{wangDisentangledRepresentationLearning2024} aims to separate semantically distinct factors to improve generalization. 
Early work used generative models~\cite{liuMultitaskAdversarialNetwork2018}, while modern strategies employ adversarial training~\cite{liuMultitaskAdversarialNetwork2018}, metric learning~\cite{chengDisentangledFeatureRepresentation2021}, or orthogonal projections~\cite{materzynskaDisentanglingVisualWritten2022}. 
In the ReID domain, this is used to separate identity-relevant signals from nuisances~\cite{liDisentanglingIdentityFeatures2024,azadActivityBiometricsPersonIdentification2024}, such as occlusion~\cite{cuiProFDPromptGuidedFeature2024} or clothing changes~\cite{liDisentanglingIdentityFeatures2024}.

While these efforts improve semantic purity, critical limitations persist. 
Without an effective interaction mechanism, isolated factors may fail to support robust cross-modal matching~\cite{azadActivityBiometricsPersonIdentification2024}. 
Reliance on manual annotations or external detectors constrains scalability~\cite{cuiProFDPromptGuidedFeature2024}, and implicit regularizers can be underconstrained, yielding spurious separations on unseen data~\cite{jiangCrossModalImplicitRelation2023}. 
Consequently, recent work emphasizes coupling disentanglement with principled interaction and independence constraints~\cite{azadActivityBiometricsPersonIdentification2024}. 
Our framework addresses this by pairing explicit supervision with model-level disentanglement to preserve cross-modal synergy.

\subsection{Feature Fusion}
Feature fusion is central to T2I-ReID~\cite{schmidtRobustCanonicalizationBootstrapped2025}, but standard methods have drawbacks. 
Cross-modal modules built on multi-head attention~\cite{yinGraFTGradualFusion2023} suffer from quadratic complexity, causing latency spikes. 
Pipelines built on CLIP~\cite{kimExtendingCLIPImageText2024} benefit from pretraining but often blur identity versus clothing cues~\cite{liPersonSearchNatural2017}. 
Other approaches like dynamic fusion~\cite{fengKnowledgeGuidedDynamicModality2024} or GNNs~\cite{li2023learninggraphneuralnetwork} introduce higher computational costs or limiting structural assumptions.

Contemporary evidence suggests that accuracy improves only when semantic disentanglement and efficient fusion advance in tandem~\cite{zhuDSSLDeepSurroundingsperson2021}. 
Practically, an ideal fusion module should respect factorized semantics (i.e., identity and clothing) and capture long-range cross-modal dependencies with linear complexity~\cite{shuSeeFinerSeeMore2022}. 
This motivates our design: BDAM supplies factor-aware representations, while a Mamba SSM fusion module models long-horizon interactions with linear complexity, enabling precise alignment without the memory and efficiency bottlenecks of standard Transformers.

\section{METHODOLOGY}
\label{sec:method}
\begin{figure*}[t]
    \centering
    \includegraphics[width=\textwidth]{figure2_2_2.pdf}
    \caption{Overview of the proposed framework. An MLLM generates identity ($f^t_{\mathrm{id}}$) and clothing ($f^t_{\mathrm{clo}}$) descriptions to supervise the BDAM. The BDAM module disentangles the input visual feature $f_i$ into identity ($f_{\mathrm{id}}$) and clothing ($f_{\mathrm{clo}}$) features, which are optimized via contrastive and decoupling losses. Finally, the fusion module integrates the visual and textual identity features ($f_{\mathrm{id}}$ and $f_{\mathrm{id}}^t$).}
    \label{fig:figure2}
\end{figure*}

\subsection{Overview}
To learn robust pedestrian representations, we propose the BDAM, which disentangles features using textual guidance.
As illustrated in Fig.~\ref{fig:figure2}, our framework comprises vision and text encoders, the core BDAM, and an efficient Mamba SSM Fusion Module.
Given an image $I \in \mathbb{R}^{B \times C \times H \times W}$, a visual encoder extracts features $f_i$.
Concurrently, we use an MLLM to generate separate identity and clothing descriptions, which a text encoder encodes into $f^t_{\mathrm{id}}$ and $f^t_{\mathrm{clo}}$.
The BDAM leverages these textual features to guide the disentanglement of $f_i$.
To enforce this separation, we employ two losses: an alignment loss ($\mathcal{L}_{\mathrm{aln}}$) to supervise the visual clothing features using clothing descriptions, and an HSIC-based loss to ensure orthogonality.
Finally, the Mamba SSM efficiently fuses the disentangled visual identity and textual semantics, enhancing the model's overall representation.

\subsection{Semantic Enhancement}
\label{sec:semantic}
\begin{figure}[ht]
    \centering
    \includegraphics[width=\columnwidth]{MLLM.pdf}
    \caption{Overview of the offline pipeline using an MLLM to generate decoupled identity and clothing descriptions. Style prompts are derived via CLIP and DBSCAN to enhance diversity.}
    \label{fig:mllm_pipeline}
\end{figure}

We employ an MLLM to automatically generate fine-grained identity and clothing descriptions for pedestrians, reducing the burden of manual annotation and enriching the available supervision. 
Fig.~\ref{fig:mllm_pipeline} illustrates this generation pipeline. Inspired by prior work on modeling annotator styles~\cite{jiangModelingThousandsHuman2025}, we first derive style categories to guide the MLLM's generation tone.

To achieve this, we use the CLIP text encoder to embed original descriptions and then cluster these style embeddings with DBSCAN. 
These discovered style categories are used to formulate textual prompts (e.g., "Use a very detailed, descriptive style"). 
A dual prompt generator, using content-specific templates (e.g., "Describe the person's identity" and "Describe the person's clothing"), then guides the MLLM to output two distinct texts per image: one description for identity (biological traits) and another for clothing (apparel, colors, and patterns). 
We apply syntax checks and validation to ensure the outputs remain grammatical and structured.

\subsection{Bidirectional Decoupled Alignment Module}
Adopting CLIP for global embedding alignment presents two key limitations: first, its limited capacity for fine-grained semantics hinders the separation of identity from clothing, and second, its holistic encoding lacks token-level cross-modal modeling, reducing robustness in complex scenes.

In this paper, we use a pre-trained ViT (ViT-B/16) as the visual encoder $E_v$~\cite{dosovitskiyImageWorth16x162021}, which outputs entangled token features $f_i \in \mathbb{R}^{B \times L \times D}$. 
A linear projection with two branches then yields preliminary identity features $f_{\text{id}}' \in \mathbb{R}^{B \times L \times D}$ and clothing features $f_{\text{clo}}' \in \mathbb{R}^{B \times L \times D}$. These are refined by multi-layer self-attention in each branch to enhance contextual awareness.

Instead of using the ViT [CLS] token, we exploit the full patch sequence and introduce cross-attention between the branches to exchange information and reinforce semantic distinctions. 
Each stream applies global average pooling (GAP) to produce $\hat{f}_\text{id}$ and $\hat{f}_\text{clo}$. 
To enable adaptive soft disentanglement, we design a gating mechanism: the concatenated global vectors are fed to a lightweight linear network with a Sigmoid output, producing a gate $g \in \mathbb{R}^{B \times D}$. 
The final features are $f_{\mathrm{id}}^{i}=g\odot \hat{f}_{\mathrm{id}}$ and $f_{\mathrm{clo}}^{i}=(1-g)\odot \hat{f}_{\mathrm{clo}}$, where $\odot$ denotes element-wise multiplication and $f_{\mathrm{id}}^{i}$ is sent to the fusion module.

To train BDAM and enforce separation, we introduce two specialized loss functions: a clothing alignment loss ($\mathcal{L}_{\mathrm{aln}}$) and an HSIC-based decoupling loss ($\mathcal{L}_{\mathrm{Decouple}}$).

The clothing alignment loss supervises the visual clothing features with the MLLM-generated clothing descriptions:
\begin{equation} \label{eq:aln}
    \mathcal{L}_{\mathrm{aln}} = -\mathbb{E}_i \left[ \log \frac{\exp(s_{ii} / \tau)}{\sum_j \exp(s_{ij} / \tau)} \right]
\end{equation}
where $s_{ij} = \hat{f}_{\mathrm{clo}}^i \cdot (f_{\mathrm{clo}}^t)^j$ is the dot-product similarity between the visual clothing feature of sample $i$ and the textual clothing feature of sample $j$, and $\tau$ is a temperature parameter.
In practice, clothing features are linearly projected to the text dimension and L2 normalized for stable similarity estimation. This alignment objective explicitly ensures the clothing stream learns accurate semantic representations, which indirectly enhances the purity of the identity features by providing clear guidance on what constitutes clothing information.
To further encourage statistical independence, we minimize a decoupling loss based on HSIC:
\begin{IEEEeqnarray}{rCl}
    \mathcal{L}_{\mathrm{Decouple}} & = & \mathrm{HSIC}(f_{\mathrm{id}}^{i}, f_{\mathrm{clo}}^{i}) \nonumber \\
    & = & \frac{1}{(N-1)^2} \mathrm{tr}(K_{\mathrm{id}} H K_{\mathrm{clo}} H)
    \label{eq:decouple}
\end{IEEEeqnarray}
Here, $f_{\mathrm{id}}^{\mathrm{i}}, f_{\mathrm{clo}}^{\mathrm{i}} \in \mathbb{R}{B \times D}$ are the gated features; $K_{\mathrm{id}} = f_{\mathrm{id}}^{\mathrm{i}} (f_{\mathrm{id}}^{\mathrm{i}})^{T}$ and $K_{\mathrm{clo}} = f_{\mathrm{clo}}^{\mathrm{i}} (f_{\mathrm{clo}}^{\mathrm{i}})^{T}$ are their kernel matrices; and $H = I_{N} - (1/N) \mathbf{1}_{N} \mathbf{1}_{N}^{T}$ is the centering matrix.
HSIC measures the statistical dependence between features; minimizing this value encourages the features to be statistically independent.

\subsection{Feature Fusion}
We introduce the Mamba SSM for efficient, semantic-sensitive feature fusion. 
The core objective is to preserve the semantic integrity of the purified identity features from both image and text, enabling a fusion that is robust to clothing variations isolated by the BDAM. 
As illustrated in Fig.~\ref{fig:fusion_module}, the process begins with an FFN performing dimensional alignment on the decoupled visual features $f_{\text{id}}^{\text{i}}$ and the textual features $f_{\text{id}}^{t}$ to generate $f_{\text{img}}$ and $f_{\text{txt}}$.

\begin{figure}[t]
    \centering
    \includegraphics[width=\linewidth]{Fusion.pdf}
    \caption{Architecture of the Mamba Fusion Module, which fuses visual ($f_{\text{id}}^{\text{i}}$) and textual ($f_{\text{id}}^{\text{t}}$) identity features while discarding clothing features ($f_{\text{clo}}^{\text{i}}$).}
    \label{fig:fusion_module}
\end{figure}

Notably, the decoupled visual clothing feature $f_{\text{clo}}^{\text{i}}$ is \textit{intentionally discarded} during fusion. This design is central to our goal: since the BDAM (supervised by $\mathcal{L}_{\text{aln}}$ and $\mathcal{L}_{\text{Decouple}}$) is tasked with purging irrelevant information into $f_{\text{clo}}^{\text{i}}$, excluding this feature forces the model to learn a representation based purely on stable identity semantics.

Following this alignment, a gating mechanism achieves dynamic weighted fusion. 
It outputs a weight vector $g_{\text{fus}} \in \mathbb{R}^{B \times 2}$, which is normalized via a SoftMax layer to produce image $W_{img}$ and text $W_{txt}$ weights ($W_{img} + W_{txt} = 1$). 
The resulting fusion is computed as: $f_{\text{fusion}} = W_{img} \cdot f_{\text{img}} + W_{txt} \cdot f_{\text{txt}}$.

The resulting $f_{\text{fusion}}$ features are then processed by the Mamba SSM to enhance cross-modal interaction. 
Leveraging its capability to model long-range dependencies, Mamba effectively captures complex sequential relationships. 
We employ a stack of Mamba layers, where each layer updates its input $f_{fusion}^{(l)}$ using a residual connection: $f_{fusion}^{(l+1)} = \text{Mamba}(f_{fusion}^{(l)}) + f_{fusion}^{(l)}$. 
This structure improves information flow. 
Finally, the output from the last Mamba layer is projected to produce the final representation, $f_{final} \in \mathbb{R}^{B \times D_{out}}$.

\subsection{Loss Function}
To achieve fine-grained alignment between modalities, we adopt the InfoNCE loss ($\mathcal{L}_{\mathrm{info}}$), which maximizes similarity for positive image-text pairs while separating negatives:
\begin{equation}
    \label{eq:info}
    \mathcal{L}_{\mathrm{info}} = -\log \frac{\exp(v_i^{\top} t_i / \tau)}{\sum_j \exp(v_i^{\top} t_j / \tau)}
\end{equation}
Here, $v_i$ is the final L2-normalized fused representation, $t_i$ is the text feature for the $i$-th identity, and $\tau$ controls distribution sharpness.

To enhance intra-modality identity discrimination, we include a triplet loss ($\mathcal{L}_{\mathrm{triplet}}$) to enforce class compactness and separation:
\begin{equation}
    \label{eq:triplet}
    \mathcal{L}_{\mathrm{triplet}} = \mathbb{E}_{(a,p,n)} \left[ \max\left( \left\| f_a - f_p \right\|_2^2 - \left\| f_a - f_n \right\|_2^2 + m, 0 \right) \right]
\end{equation}
Here, $f_a,f_p,f_n$ are the decoupled visual identity features for the anchor, positive, and negative samples, respectively, and $m$ is the margin.

In training with multiple tasks, differing loss scales can cause one task to dominate. 
We adopt GradNorm to balance the training process by dynamically adjusting the gradient norm of each task:
\begin{equation} \label{eq:gradnorm}
    \mathcal{L}_{\mathrm{GradNorm}} = \sum_k \left| \nabla_{\theta} (w_k \mathcal{L}_k) - \tilde{r}_k G_{\mathrm{ref}} \right|
\end{equation}
Here, $\nabla_{\theta} (w_k \mathcal{L}_k)$ is the gradient of the weighted loss of task $k$ w.r.t. shared parameters $\theta$. 
$\tilde{r}_k = (\mathcal{L}_k / \mathcal{L}_k^0) / \bar{r}$ is the normalized loss ratio, where $\mathcal{L}_k^0$ is the initial loss, and $G_{\mathrm{ref}}$ is a reference gradient norm.

Finally, the overall loss function is defined as follows, including a regularization term to prevent instability:
\begin{equation}
    \label{eq:total}
    \mathcal{L}_{\mathrm{Total}} = \sum_k w_k \mathcal{L}_k + \alpha \mathcal{L}_{\mathrm{GradNorm}} + \lambda \sum_k (\log w_k)^2
\end{equation}
Here, $\mathcal{L}_k$ represents the loss term for task $k$ (which includes $\mathcal{L}_{\mathrm{info}}$, $\mathcal{L}_{\mathrm{triplet}}$, $\mathcal{L}_{\mathrm{aln}}$, and $\mathcal{L}_{\mathrm{Decouple}}$), and $w_k = \exp(s_k)$ is the learnable task weight. 
The hyperparameters $\alpha$ and $\lambda$ control the GradNorm strength and regularization, respectively.

\section{Experiments}
\label{sec:experiments}
\begin{table*}[t]
    \centering
    \caption{Performance comparison with state-of-the-art methods on three benchmark datasets.}
    \label{tab:sota_comparison}
    \vspace{-0.3cm}
    \resizebox{\textwidth}{!}{
        \renewcommand{\arraystretch}{1.2}
        \begin{tabular}{l|c|cccc|cccc|cccc}
            \hline
            \multirow{2}{*}{\textbf{Method}}
                                                                        & \multirow{2}{*}{\textbf{Backbone}}
                                                                        & \multicolumn{4}{c|}{\textbf{CUHK-PEDES}}
                                                                        & \multicolumn{4}{c|}{\textbf{ICFG-PEDES}}
                                                                        & \multicolumn{4}{c}{\textbf{RSTPReid}}                                                                                                                              \\
            \cline{3-6} \cline{7-10} \cline{11-14}
                                                                        &                                          & \textbf{R-1}   & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}
                                                                        & \textbf{R-1}                             & \textbf{R-5}   & \textbf{R-10}  & \textbf{mAP}                                                                          \\
            \hline
            \multicolumn{14}{l}{\quad\small\textit{Methods with CLIP backbone:}}                                                                                                                                                             \\
            \hline
            IRRA~\cite{jiangCrossModalImplicitRelation2023}             & CLIP-ViT
                                                                        & 73.38                                    & 89.93          & 93.71          & 66.10          & 63.36        & 80.82 & 85.82 & 38.06 & 60.20 & 81.30 & 88.20 & 47.17 \\
            IRLT~\cite{liuCausalityInspiredInvariantRepresentation2024} & CLIP-ViT
                                                                        & 73.67                                    & 89.71          & 93.57          & 65.94          & 63.57        & 80.57 & 86.32 & 38.34 & 60.51 & 82.85 & 89.71 & 47.64 \\
            CFAM~\cite{zuoUFineBenchTowardsTextbased2024}               & CLIP-ViT
                                                                        & 74.46                                    & 90.19          & 94.01          & -              & 64.72        & 81.35 & 86.31 & -     & 61.49 & 82.26 & 89.23 & -     \\
            Propot~\cite{yanPrototypicalPromptingTexttoimage2024}       & CLIP-ViT
                                                                        & 74.89                                    & 89.90          & 94.17          & 67.12          & 65.12        & 81.57 & 86.97 & 42.93 & 61.87 & 83.63 & 89.70 & 47.82 \\
            RDE~\cite{qinNoisyCorrespondenceLearningTexttoImage2024}    & CLIP-ViT
                                                                        & 75.94                                    & 90.14          & 94.12          & 67.56          & 67.68        & 82.47 & 87.36 & 40.06 & 65.35 & 83.95 & 89.90 & 50.88 \\
            HAM~\cite{jiangModelingThousandsHuman2025}                  & CLIP-ViT
                                                                        & 77.99                                    & 91.34          & 95.03          & 69.72          & 69.95        & 83.88 & 88.39 & 42.72 & 72.50 & 87.70 & 91.95 & 55.47 \\
            \hline
            \multicolumn{14}{l}{\quad\small\textit{Methods with ViT backbone:}}                                                                                                                                                              \\
            \hline
            CPCL~\cite{zhengCPCLCrossModalPrototypical2024}             & ViT
                                                                        & 70.03                                    & 87.28          & 91.78          & 63.19          & 62.60        & 79.07 & 84.46 & 36.16 & 58.35 & 81.05 & 87.65 & 45.81 \\
            PDReid~\cite{liPromptDecouplingTexttoImage2024}             & ViT
                                                                        & 71.59                                    & 87.95          & 92.45          & 65.03          & 60.93        & 77.96 & 84.11 & 36.44 & 56.65 & 77.40 & 84.70 & 45.27 \\
            SSAN~\cite{dingSemanticallySelfAlignedNetwork2021}          & ViT
                                                                        & 61.37                                    & 80.15          & 86.73          & -              & 54.23        & 72.63 & 79.53 & -     & 43.50 & 67.80 & 77.15 & -     \\
            CFine~\cite{yanCLIPDrivenFinegrainedTextImage2022}          & ViT
                                                                        & 69.57                                    & 85.93          & 91.15          & -              & 60.83        & 76.55 & 82.42 & -     & 50.55 & 72.50 & 81.60 & -     \\
            IVT~\cite{shuSeeFinerSeeMore2022}                           & ViT
                                                                        & 65.59                                    & 83.11          & 89.21          & -              & 56.04        & 73.60 & 80.22 & -     & 46.70 & 70.00 & 78.80 & -     \\
            \hline
            \rowcolor{gray!20}
            \textbf{Ours}                                               & ViT
                                                                        & \textbf{79.93}                           & \textbf{92.95} & \textbf{96.47} & \textbf{72.61}
                                                                        & \textbf{68.68}                           & \textbf{84.29} & \textbf{89.74} & \textbf{41.78}
                                                                        & \textbf{74.33}                           & \textbf{88.85} & \textbf{92.95} & \textbf{57.68}                                                                        \\
            \hline
        \end{tabular}
    }
    \vspace{-0.2cm}
\end{table*}

\subsection{Implementation Details}
\label{sec:implementation_details}
\subsubsection{Datasets and Metrics}
\label{sec:datasets_metrics}
We evaluate our method on three standard benchmarks: CUHK-PEDES~\cite{liPersonSearchNatural2017}, ICFG-PEDES~\cite{zhuDSSLDeepSurroundingsperson2021}, and RSTPReid~\cite{dingSemanticallySelfAlignedNetwork2021}.
We follow their official identity-based splits and report mean Average Precision (mAP) and Rank-k accuracy (R-1, R-5, R-10).

\subsubsection{Model and Training}
\label{sec:model_training}
Our model employs a pre-trained bert-base-uncased as the text encoder and a vit-base-patch16-224 as the visual encoder.
All images are resized to $224 \times 224$ pixels.
The BDAM module disentangles visual tokens into 768-dimensional identity and clothing features.
A 2-layer Mamba fusion module (256-dimensional input/output, 16-dimensional state, 4-convolution kernel) integrates the representations.
Dropout is $0.1$ throughout.
We use the Adam optimizer (LR $1 \times 10^{-4}$, WD $1 \times 10^{-3}$) with cosine annealing.
The total loss combines InfoNCE, triplet, clothing alignment, and HSIC decoupling terms, which are dynamically balanced using GradNorm ($\alpha = 1.5$).

\subsubsection{Data Augmentation and Reporting}
\label{sec:data_augmentation}
We use clip-vit-base-patch32 and DBSCAN to find style clusters, then use ChatGPT-4 to generate decoupled identity and clothing descriptions.
All experiments are repeated with three random seeds (0, 1, 2), and we report the mean results.

\subsection{Parameter Analysis}
\label{sec:param_analysis}
\begin{figure}[htbp]
    \centering
    \includegraphics[width=\linewidth]{gate_weight_distribution.pdf}
    \caption{Learned gate weight distributions on CUHK-PEDES. (a) BDAM's identity-centric bias and (b) Fusion's balanced modality contributions.}
    \label{fig:gate_distribution}
\end{figure}
We empirically validate our gating mechanisms by analyzing the learned gate weights (Fig.~\ref{fig:gate_distribution}). 
The BDAM's dimension-level gate ($g_{\text{dis}} \in \mathbb{R}^{B \times D}$), guided by asymmetric losses (e.g., InfoNCE, HSIC) to force stream specialization, exhibits a clear identity-centric design: identity weights (mean 0.61) substantially exceed clothing weights (mean 0.38). 
This asymmetry confirms its ability to discriminate cues.

In contrast, the instance-level fusion gate ($g_{\text{fus}} \in \mathbb{R}^{B \times 2}$) maintains nearly equal modality contributions (means 0.52 and 0.48), demonstrating stable alignment without modality collapse. 
These empirical observations corroborate our theoretical design: BDAM enforces semantic disentanglement, while the fusion gate achieves dynamic equilibrium.

\subsection{Ablation Study}
We conduct systematic ablation studies on CUHK-PEDES to validate each component.

\begin{table}[H]
    \centering
    \caption{Ablation study on the BDAM module.}
    \label{tab:ablation_bdam}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                  & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o BDAM)     & 59.81              & 70.54              & 85.49              & 91.26               \\
        + BDAM                  & 66.74              & 76.27              & 89.30              & 94.02               \\
        \quad w/o Cross-Attn    & 62.56              & 71.39              & 87.05              & 92.98               \\
        \quad w/o Gate          & 65.11              & 74.63              & 88.77              & 93.56               \\
        \quad Shallow (3-layer) & 64.27              & 73.74              & 88.09              & 93.32               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\subsubsection{Disentanglement Module} Table~\ref{tab:ablation_bdam} shows that BDAM yields substantial gains.
Removing cross-attention, ablating the gating mechanism, or reducing depth all degrade performance, confirming that semantic interaction and adaptive control are essential for robust disentanglement.

\begin{table}[H]
    \centering
    \caption{Ablation study on the fusion module.}
    \label{tab:ablation_fusion}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{3.5pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method                & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        Baseline (w/o Fusion) & 59.81              & 70.54              & 85.49              & 91.26               \\
        Full Fusion           & 72.61            & 78.42              & 90.74              & 95.11               \\
        \quad w/o Mamba       & 66.89              & 75.73              & 89.06              & 93.92               \\
        \quad w/o Gate        & 68.64              & 77.58              & 90.11              & 94.87               \\
        \quad w/o Alignment   & 68.15              & 77.09              & 89.84              & 94.53               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\subsubsection{Fusion Module} Table~\ref{tab:ablation_fusion} confirms the synergy of our fusion components. 
The Mamba SSM is critical; its removal causes the largest drop, highlighting the value of long-range dependency modeling. The gating and alignment layers provide further essential gains.

\begin{table}[H]
    \centering
    \caption{Ablation study on individual loss components.}
    \label{tab:ablation_loss}
    \vspace{-0.3cm}
    \setlength{\tabcolsep}{4pt}
    \footnotesize
    \begin{tabular}{lcccc}
        \toprule
        Method         & mAP(\%) $\uparrow$ & R-1(\%) $\uparrow$ & R-5(\%) $\uparrow$ & R-10(\%) $\uparrow$ \\
        \midrule
        \rowcolor{gray!20}
        Full Model     & 72.61              & 79.93              & 92.95              & 96.47               \\
        w/o InfoNCE    & 28.14              & 36.55              & 55.21              & 65.83               \\
        w/o Triplet    & 67.22              & 74.89              & 88.15              & 93.12               \\
        w/o Alignment  & 69.15              & 76.92              & 89.53              & 94.22               \\
        w/o Decoupling & 70.03              & 77.81              & 90.11              & 94.98               \\
        w/o Gate Reg.  & 71.98              & 79.23              & 91.35              & 95.71               \\
        \bottomrule
    \end{tabular}
    \vspace{-0.2cm}
\end{table}

\subsubsection{Loss and Prompting} Table~\ref{tab:ablation_loss} validates our multi-task loss. 
InfoNCE and triplet losses are foundational. 
Crucially, the degradation without alignment or HSIC decoupling losses proves that explicit constraints are necessary to enforce identity-clothing separation. 
For prompt generation, we found density-based clustering (DBSCAN) significantly outperforms K-Me and random sampling, as it adaptively handles irregular style distributions. 
Full clustering results and t-SNE visualizations are in the supplementary material.

\subsection{Comparisons with State-of-the-Art Methods}
Table~\ref{tab:sota_comparison} compares our method against state-of-the-art approaches. 
On CUHK-PEDES and RSTPReid, our method establishes new benchmarks, substantially surpassing both ViT-based methods and the strong CLIP-based HAM baseline. 
On ICFG-PEDES, we remain highly competitive. 
Remarkably, our ViT-based method outperforms strong CLIP-based competitors, such as HAM and IRRA, by a significant margin. 
This result challenges the prevailing dominance of large-scale vision-language pre-training in T2I-ReID. 
We attribute this success to two key factors:

\textbf{Explicit Decoupling vs. Implicit Alignment:} While CLIP models implicitly learn potential correlations, our BDAM forces the model to mathematically separate clothing from identity, preventing the network from overfitting to clothing shortcuts.

\textbf{MLLM as a Superior Teacher:} The rich, fine-grained descriptions generated by the MLLM provide high-quality semantic targets that are more informative than the noisy, coarse-grained alignment learned by CLIP.
This validates that task-specific structural design combined with high-quality semantic guidance can effectively override the benefits of massive generic pre-training.

\section{Conclusion and Limitations}
\label{sec:conclusion}
We proposed a framework to address clothing interference in T2I-ReID by using a Multimodal Large Language Model (MLLM) to guide feature decoupling. 
Our Bidirectional Decoupling Alignment Module (BDAM) leverages MLLM-generated descriptions to isolate identity from clothing features, enforced by an alignment and kernel-based orthogonal loss. 
A Mamba SSM provides efficient modality fusion. 
This method achieved new state-of-the-art results on CUHK-PEDES and RSTPReid. 
Limitations include potential MLLM-generated noise and large-scale deployment overhead. 
Future work will focus on improving description reliability, enhancing inference efficiency, and testing robustness on clothing-change scenarios.

\bibliographystyle{IEEEbib}
\bibliography{icme2025references}

\end{document}